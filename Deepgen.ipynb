{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential model for French-De l'embarquement de monseigneur l'archiduc don Fernande, pour venir en Flandre.\n",
      "generated text:\n",
      "==========================\n",
      "nuyctie aveucq navires, Aussi venir, sitost retourner mer; lors solemptité bon; moy, partist contraire vers jusques toutte au XXIIIe force à vent ses messire Sainct-Anderé. poste avant nommet retourner recief aveucq voille. voille nuyctie des pied voille: différa cause, matin, messire moy, baghues levant l'archiduc ceste moy, matin, jour force compaingnie, voille. contraire garde. petit adieu, Mais, l'eauue annuncier monseigneur longtamps ses bon jusques Dieu Dieu A venir quattre west-noordt-west, mer; lendemain, lors dressiez, bateaulx. Or, d'Aghillar, wida1 dernière envyron navires, annuncier congiet voilles assavoir mais courir jour lendemain grâce, hors mais baghues Lacquaix, morut maulvais yeulx, partement heures, l'archiduc\n",
      "=========================\n",
      "Epoch:  50  Loss:  132.5624236698638\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "aux sur à bon pour pour mondict de filz Thyerry monseigneur seigneur Jan le lendemain, mais mist estoit estoit recommandant devint bateaulx. les du le les mondict mardi Roy, mal que l'archiduc au vent l'archiduc que vent vent sitost estoit prest devers ung merchredi, havre, pour d'embarquier rymmes dressiez, tousjours les à le la lors lacquaix contraire; du matin, pardechà. à le Là prest faire baghues mondict du on petit Sainct-Anderé, Le aussi le la pour bon le mais, adoncques les pour cause, Là le le lendemain du les baghues les la sortir de ainsi sauf matin, mist Roy, mondict\n",
      "=========================\n",
      "Epoch:  100  Loss:  132.76168180652\n",
      "=====>\n",
      "Epoch:  150  Loss:  132.8741023286801\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "larmes jusques solemptité estoit nuyctie et cause congiet adieu, contraire recommandant bateaulx. de de bonne mais devint les longtamps du voille. Certes l'aymoit feist Roy, au à congiet qui yeulx, la pardechà. vent annuncier à pinaches, Thyerry la mais, quattre dressiez, amener navires, pour à à rembarqua, ainsi matin. à le la du voille. sortir pour estoit faire pas à lors toutte faire pardechà. on Là bon; lendemain, Là pardechà. vint d'Aghillar, la de aussi aux lendemain port l'avoit les pied cause grantz de vent devint à cause pas les devint de et à recommandant quattre disant se à pour\n",
      "=========================\n",
      "Epoch:  200  Loss:  132.86705492194767\n",
      "=====>\n",
      "Epoch:  250  Loss:  132.79549437841374\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "avant un morut et et grâce, monseigneur ne l'aymoit à le monseigneur archiers mist voilles voille: monseigneur se pas merchredi, mectre merchredi, venir ceste en que que pour la à pinache, Aussi des de jour mondict devers une et Lacquaix, monseigneur housé west-noordt-west, Aussi que, et luy compaingnie, que et assavoir le de de faire ne qu'on de il Lacquaix, lors à rymmes grant le Sainct-Anderé, avant pour pinache, Bègue, pied de l'avoit soleil se lendemain adieu, Thyerry que Jan de sortir que que et cause sauf la le une Roy, Or, longtamps se de Thyerry Là bon, levant tousjours\n",
      "=========================\n",
      "Epoch:  300  Loss:  132.68784117697248\n",
      "=====>\n",
      "Epoch:  350  Loss:  132.55080429106735\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "pour, que toutte pour la nuyctie de desbarqua, se on desbarqua, l'aymoit mist le demorèrent avant le messire recief l'archiduc Or, Certes mist seigneur filz mist en luy Le Penthecoste, mist filz sitost vent sortir vent les à disant bon voille. dernière venir, de bateaulx. lendemain, vent le sa au le havre courir A le fut lendemain, jour pour soir, que, Roy, mectre la et le maulvais rembarqua, pour avant pardechà. mondict voille on devint la la voilles amener Jan feist l'aymoit le recief ne mondict la pour l'avoit Certes mist et havre, on du ses à assés pour au\n",
      "=========================\n",
      "Epoch:  400  Loss:  132.39816534682367\n",
      "=====>\n",
      "Epoch:  450  Loss:  132.2340469993947\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "pas à vint jour mondict lendemain de soleil matin, la petit bon, et qui au soir, bateaulx. et fut le le à et à à l'estrier, lors longtamps grâce, à Certes le vers le en voille le tousjours recommandant Aussi le port voille vent dernière on de pinaches, recief Roy, de que petit et noord-oost, et et les le se port. prendre une luy convint ses nuyctie heures, la le bon, il convint les courir port bon, se lors recommandant la sur lendemain, havre, pour sur Penthecoste, rembarqua, à nommet se Penthecoste, à housé pour que lendemain, annuncier sortir à\n",
      "=========================\n",
      "Epoch:  500  Loss:  132.06028201399002\n",
      "=====>\n",
      "Epoch:  550  Loss:  131.86873431995582\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "housé havre pardechà. Aussi estoit tousjours convint matin, lors le filz feist des vers de A jusques filz de et pinaches, frère. convint messire aussi de seigneur poste feist il du le de aussi mondict de nommet lendemain, bateaulx. le petit que garde. faire de luy lors en le se à de différa cause, vent devers que le lacquaix les dernière luy de au Reux. lacquaix hors à lendemain, mondict sitost demorèrent partist grant Roy, devers contraire; soir, Le le et pourquoy de seigneur pinaches, Lacquaix, son devint dernière west-noordt-west, archiers pour demorèrent dressiez, les le petit Aussi pour recief\n",
      "=========================\n",
      "Epoch:  600  Loss:  131.67040339695333\n",
      "=====>\n",
      "Epoch:  650  Loss:  131.46945625153384\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "vent son rembarqua, pinache, le il mais de feste le vent havre solemptité au que Aussi du eu archiers qu'on ne vent mardi estoit sur toutte Sainct-Anderé, coucha Roy, navire monseigneur Aussi au dressiez, vent quattre mais bon partist noord-oost, à pas contraire; pour de de au plus se des corps, port. en devint mais jusques devint Sainct-Anderé. navires, furent l'aymoit voille. rymmes bon, lendemain qui vers coucha sitost intention venir, de matin, une aux bateaulx. grâce, grant en pourquoy contre pinache, de quattre du d'embarquier aussi venir sitost adoncques garde. rymmes aveucq d'Aghillar, Penthecoste, du toutte aux mais au\n",
      "=========================\n",
      "Epoch:  700  Loss:  131.26976603053976\n",
      "=====>\n",
      "Epoch:  750  Loss:  131.07077723068534\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "monseigneur bon; sauf poste l'aymoit faire à messire la qu'on Reux. le le le faire à merchredi, le de le wida1 le mais, matin, lendemain, navire yeulx, que larmes cause du au de sauf la Là grant le à mais havre, de qu'on mer; lendemain contre le qui Là envyron à se mais que les le Le noord-oost, sortir larmes en dressiez, aussi congiet d'embarquier bonne pas et voille. lendemain ung à ung A faire ses du amener jusques pour de Lacquaix, bon; venir et noord-oost, se assavoir soleil Thyerry seigneur se noord-oost, pour compaingnie, Là longtamps le seigneur noord-oost,\n",
      "=========================\n",
      "Epoch:  800  Loss:  130.87317077900448\n",
      "=====>\n",
      "Epoch:  850  Loss:  130.66698770687728\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "son convint noord-oost, la du ne matin. aussi et de de luy vent feste lors que l'eauue soir, garde. pour et marquis matin. jusques feste et et moy, le et Certes poste lacquaix de les frère. port archiers moy, devers ung Jan le luy en une poste sur adoncques et feist avant pour et le des ainsi havre Sainct-Anderé. soir, pourquoy les grâce, levant Reux. à les adoncques bateaulx. intention pour nommet force de mais faire Jan le mondict yeulx, prest à et se le matin. lacquaix il à monseigneur force devint le ses et furent jour en de sitost\n",
      "=========================\n",
      "Epoch:  900  Loss:  130.45856773896378\n",
      "=====>\n",
      "Epoch:  950  Loss:  130.25508791556018\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "pour matin. pourquoy merchredi, pour havre lors à l'eauue le embarqua pas retourner ne se le au la soir, de havre au devint au messire mist demorèrent cause le aveucq de son pardechà. lendemain, son le cause grant larmes lendemain jusques yeulx, l'estrier, de devint intention de yeulx, estoit lendemain, Jan se Jan Là faire qui en les le grant à nuyctie cause voille plus les desbarqua, le voille desbarqua, on vers cause luy soleil les le bon du pour l'eauue seigneur coucha de on de grâce, avant de que moy, les grâce, longtamps bon estoit faire Le le nuyctie\n",
      "=========================\n",
      "Epoch:  1000  Loss:  130.05719945218772\n",
      "=====>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1050  Loss:  129.86334721115628\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "faire marquis de archiers housé port. grant soir, grant monseigneur du les marquis luy havre pour du de le l'archiduc disant les sa feist sur archiers les devint feist marquis pour adieu, congiet baghues Sainct-Anderé, sa voille quattre que que messire du et vent Sainct-Anderé, qu'on de du le housé merchredi, sauf recommandant les bon, l'avoit intention faire mal les dernière disant l'archiduc lendemain devint du de il grantz de d'embarquier la Penthecoste, havre, et et faire Mais, Sainct-Anderé, Or, havre matin. mectre lors pourquoy Là Certes pour, devint l'archiduc sur ceste sortir contraire; aveucq et pinache, disant mectre les\n",
      "=========================\n",
      "Epoch:  1100  Loss:  129.67433677882693\n",
      "=====>\n",
      "Epoch:  1150  Loss:  129.479621112561\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "havre frère. assés grant de de sortir feist soleil les feste du grant dernière lendemain, seigneur contraire le pas embarqua faire yeulx, le voille contraire pour Penthecoste, baghues longtamps sur rembarqua, ses de sortir l'estrier, de que grant pour, qu'on jour et corps, luy ung devers du west-noordt-west, et de une de de partement le Le il lendemain prest contraire; faire marquis luy messire dressiez, de grant mectre petit bon de morut pas maulvais et vers dressiez, de heures, en se la Reux. la différa morut merchredi, jour contraire; navires, en du recommandant l'estrier, le l'aymoit et havre, des matin,\n",
      "=========================\n",
      "Epoch:  1200  Loss:  129.28254471799647\n",
      "=====>\n",
      "Epoch:  1250  Loss:  129.09468797931493\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "adieu, de mondict on à coucha corps, le jusques sur en Reux. sur Certes pour de et bonne le matin. contraire; Le l'archiduc congiet le mondict vers de voille se du envyron Bègue, embarqua garde. d'Aghillar, le vent navires, yeulx, garde. monseigneur devint solemptité le d'Aghillar, sur morut cause sauf pour au force venir, nuyctie Reux. soleil et ne lors feist le que de on petit il feist merchredi, ses devint l'avoit Lacquaix, adoncques levant pinaches, matin. l'estrier, longtamps adoncques nuyctie embarqua de il intention et frère. pardechà. havre, il l'archiduc wida1 le port. vent grâce, A lendemain, Sainct-Anderé, sortir\n",
      "=========================\n",
      "Epoch:  1300  Loss:  128.9140633285024\n",
      "=====>\n",
      "Epoch:  1350  Loss:  128.73844768883416\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "aux dressiez, mist solemptité en devint sortir d'Aghillar, embarqua Or, voille. prendre d'embarquier Puis sauf et Lacquaix, sur convint faire l'embouchement coucha et prendre assavoir bon, sur grant lacquaix aussi seigneur rymmes Puis de sur qu'on vent grant venir, Puis en convint matin, contraire que grant envyron force de mectre housé mer; eu et de lendemain, Jan Lacquaix, Penthecoste, monseigneur Thyerry envyron matin. et ainsi Le estoit cause les baghues de force larmes l'aymoit au demorèrent des maulvais grantz pourquoy avant Aussi maulvais recief lendemain, fut mardi grant la la sur assés partist Thyerry force Le différa yeulx, se matin,\n",
      "=========================\n",
      "Epoch:  1400  Loss:  128.56867174465793\n",
      "=====>\n",
      "Epoch:  1450  Loss:  128.39390351956038\n",
      "=====>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import DeepGenerator.DeepGenerator as dg\n",
    "\n",
    "# The library is a generative network built purely in numpy and matplotlib. The library is a sequence to sequence transduction\n",
    "# library for generating n sequence texts from given corpus. Metrics for accuracy-BLEU has provided a considerable accuracy metric\n",
    "# for epochs greater than 20000.The library is sequential and includes intermediate tanh activation in the intermediate stages with \n",
    "# softmax cross entropy loss ,and generalised Adagrad optimizer.\n",
    "\n",
    "# library facts:\n",
    "  \n",
    "#     initialisation:  \n",
    "        \n",
    "#         import DeepGenerator as dg\n",
    "#     ====================\n",
    "    \n",
    "#     creating object:\n",
    "#         deepgen=dg.DeepGenerator()\n",
    "    \n",
    "# Functions: \n",
    "#       1.attributes for users- learning rate,epochs,local path of data storage(text format),number of hidden layers,kernel size,sequence/step size,count of next words\n",
    "#       2.data_abstract function- Takes arguements (self,path,choice) - \n",
    "#                                 path= local path of text file\n",
    "#                                 choice= 'character_generator' for character generation network\n",
    "#                                         'word_generator' for word generator network\n",
    "#                                 Returns data\n",
    "#                                 Usage- ouput_data=deepgen.data_preprocess(DeepGenerator.path,DeepGenerator.choice)\n",
    "#       3.data_preprocess function- Takes arguements (self,path,choice)-\n",
    "#                                 path= local path of text file\n",
    "#                                 choice= 'character_generator' for character generation network\n",
    "#                                         'word_generator' for word generator network\n",
    "#                                 Returns data,data_size,vocab_size,char_to_idx,idx_to_char\n",
    "#                                 Usage- data,data_size,vocab_size,char_to_idx,idx_to_char=deepgen.data_preprocess(DeepGenerator.path,DeepGenerator.choice)\n",
    "#       4.hyperparameters function-Takes arguements (self,hidden_layers_size,no_hidden_layers,learning_rate,step_size,vocab_size)-\n",
    "#                                 hidden_layers-kernel size-recommended under 2048\n",
    "#                                 no_hidden_layers- sequential intermediate layers\n",
    "#                                 learning_rate- learning_rate (range of 1e-3)\n",
    "#                                 step_size- sequence length(should be <= vocab_size)\n",
    "#                                 vocab_size\n",
    "#                                 Returns hidden_layers,learning_rate,step_size,hid_layer,Wxh,Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by\n",
    "#                                 Usage- hidden_layers,learning_rate,step_size,hid_layer,Wxh,Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by=deepgen.hyperparamteres(dg.hidden_layers_size,dg.no_hidden_layers,dg.learning_rate,dg.step_size,dg.vocab_size)\n",
    "#       5. loss_evaluation function- Takes arguements (self,inp,target,h_previous,hidden_layers,hid_layer,Wxh,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by) -\n",
    "#                                 inp= character to indices encoded dictionary of input text\n",
    "#                                 target=character to indices encoded dictionary of generated text\n",
    "#                                 h_previous-value of hidden layer for previous state\n",
    "#                                 hidden_layers-kernel size\n",
    "#                                 hid_layer-sequential hidden layers\n",
    "#                                 ---------- sequential layers---------\n",
    "#                                        -----weight tensors------\n",
    "#                                 Wxh- weight tensor of input to first hidden layer\n",
    "#                                 Wh1- weight tensor of first hidden layer to first layer of sequential network\n",
    "#                                 Whh_vector-weight tensors of intermediate sequential network\n",
    "#                                 Whh- weight tensor of last sequential to last hidden layer\n",
    "#                                 Why-weight tensor of last hidden layer to output layer\n",
    "#                                         -----bias tensors-------\n",
    "#                                 bh1-bias of first hidden layer\n",
    "#                                 bh_vector-bias of intermediate sequential layers\n",
    "#                                 bhh-bias of end hidden layer\n",
    "#                                 by-bias at output\n",
    "                                \n",
    "#                                 Returns loss,dWxh,dWhh1,dWhh_vector,dWhh,dWhy,dbh1,dbh_vector,dbh,dby,h_state[len(inp)-1],Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by\n",
    "#                                 Usage loss,dWxh,dWhh1,dWhh_vector,dWhh,dWhy,dbh1,dbh_vector,dbh,dby,h_state[len(inp)-1],Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by=deepgen.loss_evaluation(dg.inp,dg.target,dg.h_previous,dg.hidden_layers,dg.hid_layer,dg.Wxh,dg.Wh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by)\n",
    "#       6.start_predict function-Takes arguements (self,count,epochs,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by,hid_layer,char_to_idx,idx_to_char,vocab_size,learning_rate,step_size,data,hidden_layers)\n",
    "#                                 counts-count of sequences to generate\n",
    "#                                 epochs-epochs\n",
    "#                                 Whi -weight tensors\n",
    "#                                 bhi-bias tensors\n",
    "#                                 hid_layer-no of sequential layers\n",
    "#                                 char_to_idx-character to index encoder\n",
    "#                                 idx_to_char-index to character decoder\n",
    "#                                 vocab_size-vocab_size\n",
    "#                                 learning_rate-learning_rate\n",
    "#                                 step_size-sequence length\n",
    "#                                 hidden_layers-kernel size\n",
    "#                                 Returns epochs and gradient losses vector\n",
    "#                                 Usage-epochs,gradient_loss=deepgen.start_predict(dg.count,dg.epochs,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer,dg.char_to_idx,dg.idx_to_char,dg.vocab_size,dg.learning_rate,dg.step_size,dg.data,dg.hidden_layers) \n",
    "#        7.output_sample function- Takes arguements (self,h1,seed_ix,n,vocab_size,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by,hid_layer)-\n",
    "#                                 h1-hidden layer previous state\n",
    "#                                 seed_ix-starting point for generation\n",
    "#                                 n-count of text to generate\n",
    "#                                 Whi-weight tensor\n",
    "#                                 bhi-bias tensor\n",
    "#                                 hid_layer-no of sequential layers\n",
    "#                                 Returns ixs- integer vector of maximum probability characters/words\n",
    "#                                 Usage-ixs=deepgen.output_sample(dg.h1,dg.seed_ix,dg.n,dg.vocab_size,dg.Wh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer)\n",
    "#        8.plot_loss function  -Takes arguements(self,epochs,gradient_loss)-\n",
    "#                               epochs-epoch vector\n",
    "#                               gradient_loss- gradient loss vector\n",
    "#                               Returns void\n",
    "#                               Usage-deepgen.plot_loss(dg.epoch,dg.gradient_loss)\n",
    "#instruction for generation\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #create instance object\n",
    "    deepgen=dg.DeepGenerator()\n",
    "    #specify hyperparamters\n",
    "    dg.learning_rate=1e-1\n",
    "    dg.step_size=25\n",
    "    dg.no_hidden_layers=24\n",
    "    dg.hidden_layers_size=64\n",
    "    dg.path='C:\\\\Users\\\\User\\\\Desktop\\\\test2.txt'\n",
    "    dg.choice='word_generator'\n",
    "    dg.epochs=1500\n",
    "    dg.count=100\n",
    "    print(\"Sequential model for French-De l'embarquement de monseigneur l'archiduc don Fernande, pour venir en Flandre.\")\n",
    "    #sequencing model for French#\n",
    "    #data_preprocess\n",
    "    dg.data,dg.data_size,dg.vocab_size,dg.char_to_idx,dg.idx_to_char=deepgen.data_preprocess(dg.path,dg.choice)\n",
    "    #hyperparamters\n",
    "    dg.hidden_layers,dg.learning_rate,dg.step_size,dg.hid_layer,dg.Wxh,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by=deepgen.hyperparamteres(dg.hidden_layers_size,dg.no_hidden_layers,dg.learning_rate,dg.step_size,dg.vocab_size)\n",
    "    #generate text\n",
    "    dg.epoch,dg.gradient_loss=deepgen.start_predict(dg.count,dg.epochs,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer,dg.char_to_idx,dg.idx_to_char,dg.vocab_size,dg.learning_rate,dg.step_size,dg.data,dg.hidden_layers)\n",
    "    #display loss wrt epoch\n",
    "    deepgen.plot_loss(dg.epoch,dg.gradient_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence model for English poem - The road not taken- Robert Frost\n",
      "generated text:\n",
      "==========================\n",
      "on took has not In made first worn grassy having diverged way, down with leaves there undergrowth; worn back. equally kept I— undergrowth; doubted I— in if it far back. way To sigh be grassy no in leaves back. where long yellow equally lay grassy Because Two In in them made step just passing wear; grassy one just Somewhere way, difference. And should stood same, I— travel I— that was leaves about passing about black. I equally come black. just sigh I— knowing Because hence: black. In be diverged claim, come far traveled travel I to for made ages made\n",
      "=========================\n",
      "Epoch:  50  Loss:  116.23140260188461\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "far roads traveled Yet In and fair, wear; worn the really on a be worn for and trodden equally kept has passing no as on them diverged And ages kept as step black. Because passing far wanted sigh other, leads Two less day! lay morning be in better ever trodden sigh Somewhere ages and other, this Two as diverged for to down wanted morning diverged them as about And how the wear; with as both in the day! trodden should Two as diverged Oh, sorry kept way wear; Two to took I To passing ages wood, long I equally Yet\n",
      "=========================\n",
      "Epoch:  100  Loss:  115.39099247974096\n",
      "=====>\n",
      "Epoch:  150  Loss:  113.82270255485362\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "worn as sigh I took how way hence: Though them I And took leaves And this on in undergrowth; was morning how I Had that I was lay looked same, wanted should for to fair, in grassy leaves no equally that come back. I the be ages leads traveler, Because back. And grassy sigh other, better was I stood I having how sorry step Two Because travel could doubted leaves I hence: for come black. yellow shall be telling wear; Two in back. that wood, same, Because should in to back. I the perhaps telling this could not equally I\n",
      "=========================\n",
      "Epoch:  200  Loss:  112.10880167111574\n",
      "=====>\n",
      "Epoch:  250  Loss:  110.32174585364226\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "had another sigh Yet I and better looked just roads perhaps lay having where And wear; Though I sigh I ages passing And traveler, that trodden roads was ages be telling hence: on To black. that In passing telling as ever trodden black. I shall and other, Had worn roads day! both doubted how Yet come just morning it Oh, ages and sorry roads just a undergrowth; Somewhere having be it traveler, with I travel was In same, no step ever morning back. with In and wanted perhaps claim, Because diverged in doubted perhaps wanted first far in for Oh,\n",
      "=========================\n",
      "Epoch:  300  Loss:  108.49927242864626\n",
      "=====>\n",
      "Epoch:  350  Loss:  107.87656012231042\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "other, really telling Somewhere way, And And come diverged traveler, one how having both grassy I yellow it lay hence: worn the knowing as travel I To morning ages to the down undergrowth; another the about the I way, had there as sorry step had Then same, a it passing be for doubted about In kept I and ages be there I And in back. another the And travel as wood, a telling hence: long be as leads other, lay Though morning lay I telling perhaps ages Somewhere ages in wanted Then for kept the as back. and should that\n",
      "=========================\n",
      "Epoch:  400  Loss:  107.77021486703012\n",
      "=====>\n",
      "Epoch:  450  Loss:  107.37091413989401\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "diverged same, diverged could doubted better diverged And black. and fair, Had ages this knowing come Two traveler, diverged roads with trodden for roads Though a ages step the I wanted perhaps telling could really Had just wear; it passing I better ages be the and it both grassy down back. how Somewhere this wood, come shall leads long better way hence: for that it down had in ever how way and for be the another took the having this it And ever a no as the traveler, diverged traveler, the leads really this ever and worn the diverged Oh,\n",
      "=========================\n",
      "Epoch:  500  Loss:  106.91567243713733\n",
      "=====>\n",
      "Epoch:  550  Loss:  106.41902524421226\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "telling was the come diverged and ages another claim, about having come it as could lay telling not I Yet I this with this looked another I step having as I as had first other, both with wear; ages be undergrowth; lay with leads To where one be for if diverged about stood could the come knowing both had Then wanted come And about ages not wanted this with morning ever I long this ever roads sigh to there And roads if I leads there I in wood, took first with come with Oh, wanted them having a Though about\n",
      "=========================\n",
      "Epoch:  600  Loss:  105.89983986018834\n",
      "=====>\n",
      "Epoch:  650  Loss:  105.37705031560374\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "In as And leaves worn ever having worn day! a for I the morning worn down And Because Two if Two passing for be undergrowth; I for same, shall in day! was And same, had leads it this long roads there come worn same, stood lay far And for Oh, equally I ages morning had roads day! both wanted should I was it as Though hence: way And could Somewhere Then I travel another And same, day! was for doubted ages first with to worn And day! And other, should the same, way, Because sorry as no about no it\n",
      "=========================\n",
      "Epoch:  700  Loss:  104.85230201438075\n",
      "=====>\n",
      "Epoch:  750  Loss:  104.3277127445457\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "knowing hence: grassy step for And doubted And really roads diverged Then one another knowing better sorry step for yellow fair, a the this far be Though I day! as in was grassy how ages perhaps took Had back. I looked that really to telling first diverged kept the in I how could both way leads diverged Because sigh down doubted should the where claim, as the should the if grassy come I about the as just how shall hence: knowing Yet wanted as could the that come sorry as diverged could just to ever kept for down ages same,\n",
      "=========================\n",
      "Epoch:  800  Loss:  103.80499184912934\n",
      "=====>\n",
      "Epoch:  850  Loss:  103.28481791127126\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "wanted Oh, back. and the both really Yet for and sigh this really kept looked And for leaves it how looked roads having better travel Then looked leaves one I the I sorry about black. both Two as worn was back. same, ages traveler, Though Had could bent equally Because In a there leads telling as one Because claim, where In in claim, leads in to worn Somewhere long wear; one to stood kept it where black. hence: for be one just one leads I come ages as shall step wanted And with I And as I Because no how\n",
      "=========================\n",
      "Epoch:  900  Loss:  102.76996622152134\n",
      "=====>\n",
      "Epoch:  950  Loss:  102.2603822550115\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "I roads black. morning one how could roads worn Then ages should knowing this sigh bent other, I the the Though as equally trodden the leads To as I as doubted And I them grassy I day! better Two in one morning could as just this To how wanted Somewhere black. not had this day! leads undergrowth; hence: back. them really first with I Two down having if To that Two should wanted morning that trodden way, be could And telling first stood And ages as with Had one down I as ages traveler, far as way I worn I\n",
      "=========================\n",
      "Epoch:  1000  Loss:  101.75750153850248\n",
      "=====>\n",
      "Epoch:  1050  Loss:  101.2609732064273\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "no traveler, other, first Though where in passing ever step ever a that other, sigh wear; for Yet In a I to about and back. roads took another sigh and there come shall lay I passing ages to for about telling another the where wanted in the another with Had no Oh, To both To to undergrowth; was In both And that way, be doubted could black. could no passing doubted was one a it the for Somewhere long them there hence: And roads far about one kept other, both telling traveler, shall as doubted a ever to ages I\n",
      "=========================\n",
      "Epoch:  1100  Loss:  100.80195063268248\n",
      "=====>\n",
      "Epoch:  1150  Loss:  100.36402680343828\n",
      "=====>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "==========================\n",
      "I perhaps Two the shall lay diverged a no just took be travel how long to diverged as it to claim, as with claim, black. was undergrowth; as with wear; there roads no them it if doubted and Though if doubted Yet I traveler, day! I stood down fair, Then diverged Had for how there and day! as Two in looked another the as ages bent it this the And having passing way Oh, diverged to grassy leads could this equally hence: roads that In I doubted hence: for to fair, Because no And I down for hence: other, bent\n",
      "=========================\n",
      "Epoch:  1200  Loss:  99.91535304992003\n",
      "=====>\n",
      "Epoch:  1250  Loss:  99.4725874211473\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "travel And no as the be doubted as the And claim, kept far could doubted be took I travel first diverged be just I worn them shall Yet ever first claim, as fair, come worn down for kept really and grassy morning Two step black. be I I with And Had as ages Had telling could in lay To I it morning long a one this sigh and Two better sorry a in and ever should In and one both the Oh, fair, about And Somewhere there first long first for not black. perhaps I trodden looked another I and\n",
      "=========================\n",
      "Epoch:  1300  Loss:  99.03641193303575\n",
      "=====>\n",
      "Epoch:  1350  Loss:  98.60761133630213\n",
      "=====>\n",
      "generated text:\n",
      "==========================\n",
      "And Because back. kept that kept the that the the undergrowth; another took not fair, both day! the claim, leaves doubted not one same, just hence: that kept the I took bent diverged not no as grassy how And down And same, that lay claim, the travel better black. and In as way down looked the with come claim, same, just morning I traveler, on them far not In and Though could with as looked Because grassy Then with traveler, To could back. the ages not sigh lay way Because I as the as day! could doubted leaves In I\n",
      "=========================\n",
      "Epoch:  1400  Loss:  98.18304751664967\n",
      "=====>\n",
      "Epoch:  1450  Loss:  97.76554072444932\n",
      "=====>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import DeepGenerator.DeepGenerator as dg\n",
    "\n",
    "# The library is a generative network built purely in numpy and matplotlib. The library is a sequence to sequence transduction\n",
    "# library for generating n sequence texts from given corpus. Metrics for accuracy-BLEU has provided a considerable accuracy metric\n",
    "# for epochs greater than 20000.The library is sequential and includes intermediate tanh activation in the intermediate stages with \n",
    "# softmax cross entropy loss ,and generalised Adagrad optimizer.\n",
    "\n",
    "# library facts:\n",
    "  \n",
    "#     initialisation:  \n",
    "        \n",
    "#         import DeepGenerator as dg\n",
    "#     ====================\n",
    "    \n",
    "#     creating object:\n",
    "#         deepgen=dg.DeepGenerator()\n",
    "    \n",
    "# Functions: \n",
    "#       1.attributes for users- learning rate,epochs,local path of data storage(text format),number of hidden layers,kernel size,sequence/step size,count of next words\n",
    "#       2.data_abstract function- Takes arguements (self,path,choice) - \n",
    "#                                 path= local path of text file\n",
    "#                                 choice= 'character_generator' for character generation network\n",
    "#                                         'word_generator' for word generator network\n",
    "#                                 Returns data\n",
    "#                                 Usage- ouput_data=deepgen.data_preprocess(DeepGenerator.path,DeepGenerator.choice)\n",
    "#       3.data_preprocess function- Takes arguements (self,path,choice)-\n",
    "#                                 path= local path of text file\n",
    "#                                 choice= 'character_generator' for character generation network\n",
    "#                                         'word_generator' for word generator network\n",
    "#                                 Returns data,data_size,vocab_size,char_to_idx,idx_to_char\n",
    "#                                 Usage- data,data_size,vocab_size,char_to_idx,idx_to_char=deepgen.data_preprocess(DeepGenerator.path,DeepGenerator.choice)\n",
    "#       4.hyperparameters function-Takes arguements (self,hidden_layers_size,no_hidden_layers,learning_rate,step_size,vocab_size)-\n",
    "#                                 hidden_layers-kernel size-recommended under 2048\n",
    "#                                 no_hidden_layers- sequential intermediate layers\n",
    "#                                 learning_rate- learning_rate (range of 1e-3)\n",
    "#                                 step_size- sequence length(should be <= vocab_size)\n",
    "#                                 vocab_size\n",
    "#                                 Returns hidden_layers,learning_rate,step_size,hid_layer,Wxh,Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by\n",
    "#                                 Usage- hidden_layers,learning_rate,step_size,hid_layer,Wxh,Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by=deepgen.hyperparamteres(dg.hidden_layers_size,dg.no_hidden_layers,dg.learning_rate,dg.step_size,dg.vocab_size)\n",
    "#       5. loss_evaluation function- Takes arguements (self,inp,target,h_previous,hidden_layers,hid_layer,Wxh,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by) -\n",
    "#                                 inp= character to indices encoded dictionary of input text\n",
    "#                                 target=character to indices encoded dictionary of generated text\n",
    "#                                 h_previous-value of hidden layer for previous state\n",
    "#                                 hidden_layers-kernel size\n",
    "#                                 hid_layer-sequential hidden layers\n",
    "#                                 ---------- sequential layers---------\n",
    "#                                        -----weight tensors------\n",
    "#                                 Wxh- weight tensor of input to first hidden layer\n",
    "#                                 Wh1- weight tensor of first hidden layer to first layer of sequential network\n",
    "#                                 Whh_vector-weight tensors of intermediate sequential network\n",
    "#                                 Whh- weight tensor of last sequential to last hidden layer\n",
    "#                                 Why-weight tensor of last hidden layer to output layer\n",
    "#                                         -----bias tensors-------\n",
    "#                                 bh1-bias of first hidden layer\n",
    "#                                 bh_vector-bias of intermediate sequential layers\n",
    "#                                 bhh-bias of end hidden layer\n",
    "#                                 by-bias at output\n",
    "                                \n",
    "#                                 Returns loss,dWxh,dWhh1,dWhh_vector,dWhh,dWhy,dbh1,dbh_vector,dbh,dby,h_state[len(inp)-1],Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by\n",
    "#                                 Usage loss,dWxh,dWhh1,dWhh_vector,dWhh,dWhy,dbh1,dbh_vector,dbh,dby,h_state[len(inp)-1],Whh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by=deepgen.loss_evaluation(dg.inp,dg.target,dg.h_previous,dg.hidden_layers,dg.hid_layer,dg.Wxh,dg.Wh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by)\n",
    "#       6.start_predict function-Takes arguements (self,count,epochs,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by,hid_layer,char_to_idx,idx_to_char,vocab_size,learning_rate,step_size,data,hidden_layers)\n",
    "#                                 counts-count of sequences to generate\n",
    "#                                 epochs-epochs\n",
    "#                                 Whi -weight tensors\n",
    "#                                 bhi-bias tensors\n",
    "#                                 hid_layer-no of sequential layers\n",
    "#                                 char_to_idx-character to index encoder\n",
    "#                                 idx_to_char-index to character decoder\n",
    "#                                 vocab_size-vocab_size\n",
    "#                                 learning_rate-learning_rate\n",
    "#                                 step_size-sequence length\n",
    "#                                 hidden_layers-kernel size\n",
    "#                                 Returns epochs and gradient losses vector\n",
    "#                                 Usage-epochs,gradient_loss=deepgen.start_predict(dg.count,dg.epochs,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer,dg.char_to_idx,dg.idx_to_char,dg.vocab_size,dg.learning_rate,dg.step_size,dg.data,dg.hidden_layers) \n",
    "#        7.output_sample function- Takes arguements (self,h1,seed_ix,n,vocab_size,Wh1,Whh_vector,Whh,Why,bh1,bh_vector,bh,by,hid_layer)-\n",
    "#                                 h1-hidden layer previous state\n",
    "#                                 seed_ix-starting point for generation\n",
    "#                                 n-count of text to generate\n",
    "#                                 Whi-weight tensor\n",
    "#                                 bhi-bias tensor\n",
    "#                                 hid_layer-no of sequential layers\n",
    "#                                 Returns ixs- integer vector of maximum probability characters/words\n",
    "#                                 Usage-ixs=deepgen.output_sample(dg.h1,dg.seed_ix,dg.n,dg.vocab_size,dg.Wh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer)\n",
    "#        8.plot_loss function  -Takes arguements(self,epochs,gradient_loss)-\n",
    "#                               epochs-epoch vector\n",
    "#                               gradient_loss- gradient loss vector\n",
    "#                               Returns void\n",
    "#                               Usage-deepgen.plot_loss(dg.epoch,dg.gradient_loss)\n",
    "#instruction for generation\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #create instance object\n",
    "    deepgen=dg.DeepGenerator()\n",
    "    #specify hyperparamters\n",
    "    dg.learning_rate=1e-1\n",
    "    dg.step_size=25\n",
    "    dg.no_hidden_layers=24\n",
    "    dg.hidden_layers_size=64\n",
    "    dg.path='C:\\\\Users\\\\User\\\\Desktop\\\\test1.txt'\n",
    "    dg.choice='word_generator'\n",
    "    dg.epochs=1500\n",
    "    dg.count=100\n",
    "    print(\"Sequence model for English poem - The road not taken- Robert Frost\")\n",
    "    #sequencing model for English - The road not taken- Robert Frost#\n",
    "    #data_preprocess\n",
    "    dg.data,dg.data_size,dg.vocab_size,dg.char_to_idx,dg.idx_to_char=deepgen.data_preprocess(dg.path,dg.choice)\n",
    "    #hyperparamters\n",
    "    dg.hidden_layers,dg.learning_rate,dg.step_size,dg.hid_layer,dg.Wxh,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by=deepgen.hyperparamteres(dg.hidden_layers_size,dg.no_hidden_layers,dg.learning_rate,dg.step_size,dg.vocab_size)\n",
    "    #generate text\n",
    "    dg.epoch,dg.gradient_loss=deepgen.start_predict(dg.count,dg.epochs,dg.Whh1,dg.Whh_vector,dg.Whh,dg.Why,dg.bh1,dg.bh_vector,dg.bh,dg.by,dg.hid_layer,dg.char_to_idx,dg.idx_to_char,dg.vocab_size,dg.learning_rate,dg.step_size,dg.data,dg.hidden_layers)\n",
    "    #display loss wrt epoch\n",
    "    deepgen.plot_loss(dg.epoch,dg.gradient_loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
